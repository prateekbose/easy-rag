{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a0ea27",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047948c1",
   "metadata": {},
   "source": [
    "### Basic Syntax without external modules\n",
    "This code snippet processes data files located in a directory named `data`. It iterates through each file in the directory, reads its content, and then divides the content into smaller chunks of size `512 bytes`. Each chunk is then processed by a function called `process`, which could perform various analyses or transformations on the data. The script uses basic file handling techniques with Python's built-in `os` module to navigate directories and read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample string that has been expanded to become much longer. It includes various words and phrases designed to make up the required word count. The goal here is to generate content that is coherent yet lengthy, showcasing the ability to', 'produce substantial amounts of text on demand.', 'Another example for testing purposes, this string aims to achieve similar length as the previous one. It features a diverse range of vocabulary and sentence structures to ensure it meets the 1000-word requirement while maintaining readability and relevance.']\n"
     ]
    }
   ],
   "source": [
    "def process(chunk):\n",
    "    print(chunk)\n",
    "\n",
    "def tokenize_documents(strings, max_tokens_per_chunk=50):\n",
    "    \"\"\"\n",
    "        Tokenizes a list of strings into individual words and groups them into chunks based on max token limit for each string.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for text in strings:\n",
    "        tokens = text.split(' ')\n",
    "        current_chunk = []\n",
    "        chunks = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if len(current_chunk) + len(token) + 1 <= max_tokens_per_chunk:  # +1 for space\n",
    "                current_chunk.append(token)\n",
    "            else:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [token]\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# List of strings to process\n",
    "string_list = [\n",
    "    \"This is a sample string that has been expanded to become much longer. It includes various words and phrases designed to make up the required word count. The goal here is to generate content that is coherent yet lengthy, showcasing the ability to produce substantial amounts of text on demand.\",\n",
    "    \"Another example for testing purposes, this string aims to achieve similar length as the previous one. It features a diverse range of vocabulary and sentence structures to ensure it meets the 1000-word requirement while maintaining readability and relevance.\"\n",
    "]\n",
    "\n",
    "# Tokenize data into smaller parts\n",
    "chunks = tokenize_documents(string_list)\n",
    "\n",
    "process(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a77a9",
   "metadata": {},
   "source": [
    "### Adding Recursive Text Splitting\n",
    "This enhanced Python code snippet still processes data files located in a directory named `data`. It introduces a more sophisticated tokenization function called `tokenize` that splits text into smaller, meaningful chunks based on specified delimiter characters such as spaces, periods, commas, semicolons, and newlines. This approach ensures that each chunk maintains linguistic coherence by respecting sentence boundaries and punctuation marks. The script iterates through each file in the directory, reads its content, tokenizes it using the `tokenize` function, and then processes each token using a function called `process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df821f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample string that has been expanded to become much\n",
      "longer It includes various words and phrases designed to\n",
      "make up the required word count The goal here is to generate\n",
      "content that is coherent yet lengthy showcasing the\n",
      "ability to produce substantial amounts of text on demand\n",
      "Another example for testing purposes this string aims to\n",
      "achieve similar length as the previous one It features a\n",
      "diverse range of vocabulary and sentence structures to\n",
      "ensure it meets the 1000-word requirement while\n",
      "maintaining readability and relevance\n"
     ]
    }
   ],
   "source": [
    "def process(chunk):\n",
    "    print(' '.join(chunk))\n",
    "\n",
    "def tokenize_documents(strings, delimiters, max_tokens_per_chunk=50):\n",
    "    \"\"\"\n",
    "        Tokenizes a list of strings into smaller chunks based on specified delimiters and maximum token limit per chunk.\n",
    "        Uses recursive splitting to handle multiple delimiters and ensures each chunk maintains linguistic coherence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def recursive_split(s, delims):\n",
    "        if not delims:\n",
    "            return [s]\n",
    "        delim = delims[0]\n",
    "        parts = s.split(delim)\n",
    "        result = []\n",
    "        for part in parts:\n",
    "            if(part):\n",
    "                result.extend(recursive_split(part, delims[1:]))\n",
    "        return result\n",
    "    \n",
    "    def create_chunks(tokens, max_tokens):\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for token in tokens:\n",
    "            token_length = len(token)\n",
    "            if current_length + token_length > max_tokens:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = [token]\n",
    "                current_length = token_length\n",
    "            else:\n",
    "                current_chunk.append(token)\n",
    "                current_length += token_length\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    all_chunks = []\n",
    "    for string in strings:\n",
    "        tokens = recursive_split(string, delimiters)\n",
    "        chunks = create_chunks(tokens, max_tokens_per_chunk)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# List of strings to process\n",
    "string_list = [\n",
    "    \"This is a sample string that has been expanded to become much longer. It includes various words and phrases designed to make up the required word count. The goal here is to generate content that is coherent yet lengthy, showcasing the ability to produce substantial amounts of text on demand.\",\n",
    "    \"Another example for testing purposes, this string aims to achieve similar length as the previous one. It features a diverse range of vocabulary and sentence structures to ensure it meets the 1000-word requirement while maintaining readability and relevance.\"\n",
    "]\n",
    "\n",
    "# Define characters to split the text\n",
    "split_chars = [' ', '.', ',', ';', '\\n']  # Example characters\n",
    "# Tokenize data into smaller parts\n",
    "tokens = tokenize_documents(string_list, delimiters=split_chars)\n",
    "\n",
    "for token in tokens:\n",
    "    # Process each token (e.g. perform some analysis or transformation)\n",
    "    processed_token = process(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d18bd1",
   "metadata": {},
   "source": [
    "### Using LangChain for simplicity\n",
    "This Python code snippet uses the LangChain library to enhance the chunking process. It leverages the `DirectoryLoader` to load data from a directory named `data`, splitting each document into manageable parts. The `RecursiveCharacterTextSplitter` is then used to create chunks of size 512 characters, with an overlap of 20% (102 characters) to ensure continuity between chunks. By specifying multiple separators like paragraphs, lines, spaces, and characters, the module can intelligently breaks down text while maintaining linguistic coherence. This approach simplifies the chunking process and improves the quality of the resulting chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24d584c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample string that has been expanded to\n",
      "to become much longer. It includes various words\n",
      "words and phrases designed to make up the\n",
      "up the required word count. The goal here is to\n",
      "is to generate content that is coherent yet\n",
      "yet lengthy, showcasing the ability to produce\n",
      "produce substantial amounts of text on demand.\n",
      "Another example for testing purposes, this string\n",
      "string aims to achieve similar length as the\n",
      "as the previous one. It features a diverse range\n",
      "range of vocabulary and sentence structures to\n",
      "to ensure it meets the 1000-word requirement\n",
      "while maintaining readability and relevance.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain &> /dev/null\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def process(chunk):\n",
    "    print(chunk)\n",
    "\n",
    "# Load data from directory 'data' under the same directorty\n",
    "string_list = [\n",
    "    \"This is a sample string that has been expanded to become much longer. It includes various words and phrases designed to make up the required word count. The goal here is to generate content that is coherent yet lengthy, showcasing the ability to produce substantial amounts of text on demand.\",\n",
    "    \"Another example for testing purposes, this string aims to achieve similar length as the previous one. It features a diverse range of vocabulary and sentence structures to ensure it meets the 1000-word requirement while maintaining readability and relevance.\"\n",
    "]\n",
    "\n",
    "CHUNK_SIZE = 50\n",
    "CHUNK_OVERLAP = int(CHUNK_SIZE * 0.2)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[' ', '.', ',', ';', '\\n'],\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents([Document(page_content=item) for item in string_list])\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
