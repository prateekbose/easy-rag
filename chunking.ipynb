{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a0ea27",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047948c1",
   "metadata": {},
   "source": [
    "### Basic Syntax without external modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e072da",
   "metadata": {},
   "source": [
    "This code snippet processes data files located in a directory named `data`. It iterates through each file in the directory, reads its content, and then divides the content into smaller chunks of size `512 bytes`. Each chunk is then processed by a function called `process`, which could perform various analyses or transformations on the data. The script uses basic file handling techniques with Python's built-in `os` module to navigate directories and read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_directory = 'data'\n",
    "\n",
    "for filename in os.listdir(data_directory):\n",
    "    filepath = os.path.join(data_directory, filename)\n",
    "\n",
    "    # Load data from file\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Chunk data into smaller parts\n",
    "    chunk_size = 512\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data[i:i+chunk_size]\n",
    "\n",
    "        # Process chunk (e.g. perform some analysis or transformation)\n",
    "        processed_chunk = process(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a77a9",
   "metadata": {},
   "source": [
    "### Adding Recursive Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a83e1",
   "metadata": {},
   "source": [
    "This enhanced Python code snippet still processes data files located in a directory named `data`. It introduces a more sophisticated tokenization function called `tokenize` that splits text into smaller, meaningful chunks based on specified delimiter characters such as spaces, periods, commas, semicolons, and newlines. This approach ensures that each chunk maintains linguistic coherence by respecting sentence boundaries and punctuation marks. The script iterates through each file in the directory, reads its content, tokenizes it using the `tokenize` function, and then processes each token using a function called `process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df821f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_directory = 'data'\n",
    "\n",
    "def tokenize(text, max_token_size=512, split_chars=None):\n",
    "    if len(text) <= max_token_size:\n",
    "        return [text]\n",
    "    \n",
    "    # Split the text based on specified characters\n",
    "    if split_chars is not None and any(char in text for char in split_chars):\n",
    "        for char in split_chars:\n",
    "            if char in text:\n",
    "                parts = text.split(char)\n",
    "                tokens = []\n",
    "                for part in parts:\n",
    "                    tokens.extend(tokenize(part, max_token_size, split_chars))\n",
    "                return tokens\n",
    "    \n",
    "    mid_point = len(text) // 2\n",
    "    left_tokens = tokenize(text[:mid_point], max_token_size, split_chars)\n",
    "    right_tokens = tokenize(text[mid_point:], max_token_size, split_chars)\n",
    "    \n",
    "    return left_tokens + right_tokens\n",
    "\n",
    "for filename in os.listdir(data_directory):\n",
    "    filepath = os.path.join(data_directory, filename)\n",
    "\n",
    "    # Load data from file\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Define characters to split the text\n",
    "    split_chars = [' ', '.', ',', ';', '\\n']  # Example characters\n",
    "\n",
    "    # Tokenize data into smaller parts\n",
    "    tokens = tokenize(data, split_chars=split_chars)\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Process each token (e.g. perform some analysis or transformation)\n",
    "        processed_token = process(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d18bd1",
   "metadata": {},
   "source": [
    "### Using LangChain for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924939b",
   "metadata": {},
   "source": [
    "This Python code snippet uses the LangChain library to enhance the chunking process. It leverages the `DirectoryLoader` to load data from a directory named `data`, splitting each document into manageable parts. The `RecursiveCharacterTextSplitter` is then used to create chunks of size 512 characters, with an overlap of 20% (102 characters) to ensure continuity between chunks. By specifying multiple separators like paragraphs, lines, spaces, and characters, the module can intelligently breaks down text while maintaining linguistic coherence. This approach simplifies the chunking process and improves the quality of the resulting chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d584c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load data from directory 'data' under the same directorty\n",
    "loader = DirectoryLoader('data')\n",
    "\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = int(CHUNK_SIZE * 0.2)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
